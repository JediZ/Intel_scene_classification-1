{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "DATA_DIR = 'data/train-scene classification'\n",
    "MODEL_DIR = \"saved_model/\"\n",
    "\n",
    "IMG_DIR = os.path.join(DATA_DIR, 'train')\n",
    "\n",
    "TRAIN_CSV = os.path.join(DATA_DIR, 'train.csv')\n",
    "TEST_CSV = \"data/test_WyRytb0.csv\"\n",
    "\n",
    "RESULTS = \"results/\"\n",
    "\n",
    "image = 'image_name'\n",
    "label = 'label'\n",
    "\n",
    "numpy_dir = \"numpy_outs/\"\n",
    "\n",
    "H = W = 150\n",
    "ORIG_H = ORIG_W = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utlities import stratified_split\n",
    "#generate metadata\n",
    "# stratified_split(TRAIN_CSV,\"k_fold_metadata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(TEST_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 150)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "img = Image.open(\"data/train-scene classification/train/\"+\"2.jpg\")\n",
    "img.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision.transforms.functional as Func\n",
    "from torch.utils import data\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "import tqdm\n",
    "from torch.autograd import Variable\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class AVDataset(data.Dataset):\n",
    "    def __init__(self, IMG_DIR, file_list, is_test = False, transform=None):\n",
    "        self.is_test = is_test\n",
    "        self.root_path = IMG_DIR\n",
    "        self.file_list = file_list\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if index not in range(0, len(self.file_list)):\n",
    "            return self.__getitem__(np.random.randint(0, self.__len__()))\n",
    "        \n",
    "        file_id = self.file_list[index]\n",
    "        \n",
    "        image_path = os.path.join(self.root_path, file_id[0])\n",
    "        \n",
    "        image = load_image(image_path)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        if self.is_test==True:\n",
    "            return image\n",
    "        else:\n",
    "            return image,file_id[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                        std=[0.229, 0.224, 0.225])\n",
    "                                   ])\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=1.0),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                        std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "train_transforms2 = transforms.Compose([\n",
    "    transforms.ColorJitter(brightness=0.4, contrast=0.2, saturation=0.5, hue=0.3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                        std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "train_transforms3 = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=1.0),\n",
    "    transforms.ColorJitter(brightness=0.4, contrast=0.2, saturation=0.5, hue=0.3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                        std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "def load_image(path):\n",
    "    img = cv2.imread(str(path))\n",
    "    img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
    "    if img.shape != (H,W,3):\n",
    "        img = cv2.resize(img, (H,W), interpolation = cv2.INTER_AREA)\n",
    "    return Image.fromarray(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_file_list(dataframe):\n",
    "    return [tuple(x) for x in dataframe.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from pathlib import Path\n",
    "from torch.nn import functional as F\n",
    "from torchvision import models\n",
    "from torchvision.transforms import ToTensor, Normalize, Compose\n",
    "import cv2\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cnn_finetune import make_model\n",
    "model = make_model('resnet34', num_classes=6, pretrained=True, input_size=(150, 150))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import models\n",
    "from torchsummary import summary\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "vgg = model.to(device)\n",
    "\n",
    "\n",
    "# summary(vgg, (3, 150, 150))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best,filename):\n",
    "#     filename='saved_model/best_acc.pth.tar'\n",
    "    \"\"\"Save checkpoint if a new best is achieved\"\"\"\n",
    "    if is_best:\n",
    "        print (\"=> Saving a new best\")\n",
    "        torch.save(state, filename)  # save checkpoint\n",
    "    else:\n",
    "        print (\"=> Validation loss did not improve\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report , confusion_matrix\n",
    "def train_data(train_dataloader,val_dataloader, filename):\n",
    "    best_acc=0.0\n",
    "    model = make_model('resnet50', num_classes=6, pretrained=True, input_size=(150, 150), dropout_p=0.5)\n",
    "    model = model.to(device)\n",
    "    learning_rate = 1e-3\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate,momentum=0.9)\n",
    "    criterion =  nn.CrossEntropyLoss()\n",
    "    epoch=10\n",
    "    for e in range(epoch):\n",
    "    #     learning_rate = cyclic_lr(e)\n",
    "        print(\"learning_rate\",learning_rate)\n",
    "        train_loss = []\n",
    "        train_preds = []\n",
    "        train_truth = []\n",
    "        for image, mask in tqdm.tqdm(train_dataloader):\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            image = image.type(torch.FloatTensor).cuda()\n",
    "            y_pred = model(Variable(image))\n",
    "            loss = criterion(y_pred, Variable(mask.cuda()))\n",
    "\n",
    "            _,predicted = torch.max(y_pred,1)\n",
    "            train_preds.extend(predicted.cpu().data.numpy())\n",
    "            train_truth.extend(mask.cpu().data.numpy())\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            train_loss.append(loss.item())\n",
    "\n",
    "        val_loss = []\n",
    "        val_preds = []\n",
    "        val_truth = []\n",
    "        for image, mask in val_dataloader:\n",
    "            image = image.type(torch.FloatTensor).cuda()\n",
    "            y_pred = model(Variable(image))\n",
    "\n",
    "            _,predicted = torch.max(y_pred,1)\n",
    "            val_preds.extend(predicted.cpu().data.numpy())\n",
    "            val_truth.extend(mask.cpu().data.numpy())\n",
    "\n",
    "            loss = criterion(y_pred, Variable(mask.cuda()))\n",
    "            val_loss.append(loss.item())\n",
    "        \n",
    "        train_acc = accuracy_score(train_truth,train_preds)\n",
    "        val_acc = accuracy_score(val_truth,val_preds)\n",
    "        print(\"Epoch: %d, Train loss: %.3f,  Val loss: %.3f\" % (e, np.mean(train_loss), np.mean(val_loss)))\n",
    "        print(\"Train acc: %.3f, Val acc: %.3f\" %(train_acc, val_acc))\n",
    "#         print(\"Classification report\" , classification_report(val_truth, val_preds))\n",
    "        print(\"Confusion report\" , confusion_matrix(val_truth, val_preds))\n",
    "        \n",
    "        is_best = bool(val_acc>best_acc)\n",
    "        best_acc = max(val_acc,best_acc)\n",
    "        save_checkpoint({\n",
    "            'epoch': e + 1,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'best_acc': best_acc\n",
    "        }, is_best, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************** Fold 0 **********\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'IMG_DIR' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-2c20188a4973>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mfile_list_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_file_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mfile_list_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_file_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     dataset = torch.utils.data.ConcatDataset([AVDataset(IMG_DIR, file_list_train, transform=train_transforms),\n\u001b[0m\u001b[1;32m      8\u001b[0m                                               \u001b[0mAVDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIMG_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_list_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mno_transforms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                                               \u001b[0mAVDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIMG_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_list_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_transforms2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'IMG_DIR' is not defined"
     ]
    }
   ],
   "source": [
    "for i in range(0,10):\n",
    "    print(\"*\"*20, \"Fold\", i,\"*\"*10)\n",
    "    train = pd.read_csv(\"k_fold_metadata/train_{}.csv\".format(i))\n",
    "    val = pd.read_csv(\"k_fold_metadata/val_{}.csv\".format(i))\n",
    "    file_list_train = create_file_list(train)\n",
    "    file_list_val = create_file_list(val)\n",
    "    dataset = torch.utils.data.ConcatDataset([AVDataset(IMG_DIR, file_list_train, transform=train_transforms),\n",
    "                                              AVDataset(IMG_DIR, file_list_train, transform=no_transforms),\n",
    "                                              AVDataset(IMG_DIR, file_list_train, transform=train_transforms2),\n",
    "                                              AVDataset(IMG_DIR, file_list_train, transform=train_transforms3)])\n",
    "    val_dataset = torch.utils.data.ConcatDataset([AVDataset(IMG_DIR, file_list_val,transform=no_transforms),\n",
    "                                                  AVDataset(IMG_DIR, file_list_val,transform=train_transforms),\n",
    "                                                  AVDataset(IMG_DIR, file_list_val,transform=train_transforms2),\n",
    "                                                  AVDataset(IMG_DIR, file_list_val,transform=train_transforms3)])\n",
    "    train_dataloader = data.DataLoader(dataset, batch_size = 16,shuffle=True, num_workers=8)\n",
    "    val_dataloader = data.DataLoader(val_dataset, batch_size = 8, shuffle = False, num_workers=4)\n",
    "    train_data(train_dataloader, val_dataloader, \"saved_model/resnet50/model_{}.pth.tar\".format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7301\n"
     ]
    }
   ],
   "source": [
    "test_file_list = create_file_list(test_data)\n",
    "print(len(test_file_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_model(model, weight_path, dataloader_test, result_path, fname):\n",
    "    submission=test_data\n",
    "    all_predictions = []\n",
    "    numpy_preds = []\n",
    "    checkpoint = torch.load(weight_path)\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    best_accuracy = checkpoint['best_acc']\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    model.eval()\n",
    "    for image in tqdm.tqdm(dataloader_test):\n",
    "        image = image.type(torch.FloatTensor).cuda()\n",
    "        y_pred = F.softmax(model(Variable(image)))\n",
    "        _,predicted = torch.max(y_pred,1)\n",
    "        all_predictions.extend(predicted.cpu().data.numpy())\n",
    "        numpy_preds.extend(y_pred.cpu().data.numpy())\n",
    "    with open(numpy_dir+\"{}{}_{}.pkl\".format(result_path,fname,i), 'wb') as f:\n",
    "        pickle.dump(numpy_preds, f)\n",
    "    submission[\"label\"]=all_predictions\n",
    "    submission.to_csv(RESULTS+\"{}{}_{}.csv\".format(result_path,fname,i),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/913 [00:00<?, ?it/s]/home/mukesh/miniconda3/envs/tgs_torch/lib/python3.6/site-packages/ipykernel_launcher.py:12: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  if sys.path[0] == '':\n",
      "100%|██████████| 913/913 [00:49<00:00, 18.60it/s]\n",
      "100%|██████████| 913/913 [00:50<00:00, 18.15it/s]\n",
      "100%|██████████| 913/913 [00:51<00:00, 17.74it/s]\n",
      "100%|██████████| 913/913 [00:51<00:00, 17.87it/s]\n",
      "100%|██████████| 913/913 [00:51<00:00, 17.69it/s]\n",
      "100%|██████████| 913/913 [00:50<00:00, 18.13it/s]\n",
      "100%|██████████| 913/913 [00:51<00:00, 17.80it/s]\n",
      "100%|██████████| 913/913 [00:51<00:00, 17.75it/s]\n",
      "100%|██████████| 913/913 [00:51<00:00, 17.65it/s]\n",
      "100%|██████████| 913/913 [00:51<00:00, 17.78it/s]\n",
      "100%|██████████| 913/913 [00:52<00:00, 17.48it/s]\n",
      "100%|██████████| 913/913 [00:51<00:00, 17.75it/s]\n",
      "100%|██████████| 913/913 [00:53<00:00, 17.10it/s]\n",
      "100%|██████████| 913/913 [00:51<00:00, 17.56it/s]\n",
      "100%|██████████| 913/913 [00:52<00:00, 17.54it/s]\n",
      "100%|██████████| 913/913 [00:52<00:00, 17.45it/s]\n",
      "100%|██████████| 913/913 [00:52<00:00, 17.35it/s]\n",
      "100%|██████████| 913/913 [00:51<00:00, 17.69it/s]\n",
      "100%|██████████| 913/913 [00:54<00:00, 16.88it/s]\n",
      "100%|██████████| 913/913 [00:54<00:00, 16.64it/s]\n",
      "100%|██████████| 913/913 [01:12<00:00, 12.63it/s]\n",
      "100%|██████████| 913/913 [01:11<00:00, 12.70it/s]\n",
      "100%|██████████| 913/913 [01:12<00:00, 12.68it/s]\n",
      "100%|██████████| 913/913 [01:11<00:00, 12.83it/s]\n",
      "100%|██████████| 913/913 [01:10<00:00, 13.03it/s]\n",
      "100%|██████████| 913/913 [01:10<00:00, 12.93it/s]\n",
      "100%|██████████| 913/913 [01:09<00:00, 13.16it/s]\n",
      "100%|██████████| 913/913 [01:10<00:00, 13.01it/s]\n",
      "100%|██████████| 913/913 [01:10<00:00, 12.94it/s]\n",
      "100%|██████████| 913/913 [01:10<00:00, 12.98it/s]\n",
      "100%|██████████| 913/913 [01:09<00:00, 13.11it/s]\n",
      "100%|██████████| 913/913 [01:10<00:00, 12.87it/s]\n",
      "100%|██████████| 913/913 [01:06<00:00, 13.76it/s]\n",
      "100%|██████████| 913/913 [01:06<00:00, 13.72it/s]\n",
      "100%|██████████| 913/913 [01:06<00:00, 13.71it/s]\n",
      "100%|██████████| 913/913 [01:07<00:00, 13.57it/s]\n",
      "100%|██████████| 913/913 [01:06<00:00, 13.69it/s]\n",
      "100%|██████████| 913/913 [01:06<00:00, 13.75it/s]\n",
      "100%|██████████| 913/913 [01:06<00:00, 13.71it/s]\n",
      "100%|██████████| 913/913 [01:06<00:00, 13.63it/s]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"se_resnet50\"\n",
    "model = make_model(model_name, num_classes=6, pretrained=True, input_size=(150, 150))#, dropout_p=0.5)\n",
    "model = model.to(\"cuda\")\n",
    "saved_folders = \"{}/\".format(model_name)#, resnet34\"resnet50/\", \"se_resnet50/\"]:\n",
    "for trans, name in zip([no_transforms, train_transforms, train_transforms2, train_transforms3],\n",
    "             [\"no_transforms\", \"train_transforms\", \"train_transforms2\", \"train_transforms3\"]):\n",
    "    test_dataset = AVDataset(IMG_DIR, test_file_list, is_test = True, transform=trans)\n",
    "    dataloader_test = data.DataLoader(test_dataset, batch_size = 8)\n",
    "    for i,weights in enumerate(os.listdir(MODEL_DIR+saved_folders)):\n",
    "        predict_model(model, MODEL_DIR+saved_folders+weights, dataloader_test, saved_folders, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
